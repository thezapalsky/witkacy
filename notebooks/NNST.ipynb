{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPv3V5uxGJDF5b8C3ved9M9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98ZEY98Vlz7_","executionInfo":{"status":"ok","timestamp":1668198263694,"user_tz":-60,"elapsed":26259,"user":{"displayName":"Mikołaj Zapalski","userId":"12241681530326187312"}},"outputId":"17c98869-6292-4c11-93b8-f11cc760d038"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# code source\n","# https://github.com/nkolkin13/NeuralNeighborStyleTransfer\n","\n","import sys\n","import os\n","import random\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","NST_path = \"/content/drive/MyDrive/NST/\"\n","\n","def listdir_nohidden(path):\n","    listed = []\n","    for f in os.listdir(path):\n","        if not f.startswith('.'):\n","            listed.append(f)\n","    return listed\n","\n","style_names = listdir_nohidden(NST_path+\"dataset/style\")\n","content_names = listdir_nohidden(NST_path+\"dataset/content\")"]},{"cell_type":"code","source":["%%time\n","\n","c_pairs = [\"content14.jpg\", \"content16.jpg\", \"content22.jpg\", \"content25.jpg\", \"content27.jpg\"]\n","s_pairs = [\"witkacy2.jpg\", \"matejko5.jpg\", \"beksinski1.jpg\", \"wyspianski1.jpg\", \"witkacy5.jpg\"]\n","\n","#for i in range(1):\n","#c_name = random.choice(content_names)\n","#s_name = random.choice(style_names)\n","s_name =  \"matejko5.jpg\"#s_pairs[i]\n","c_name = \"content16.jpg\"\n","print(c_name, s_name)\n","\n","run_path = NST_path + 'NNST/styleTransfer.py'\n","style_path = NST_path + 'dataset/style/'+s_name\n","content_path = NST_path + 'dataset/content/'+c_name\n","output_path = NST_path + 'NNST/outputs/'+c_name +'x'+s_name+'.jpg'\n","#os.system(\"python \"+run_path +\" --content_path \"+content_path +\" --style_path \"+style_path+\" --output_path \"+output_path)\n","!python $run_path --content_path $content_path --style_path $style_path --output_path $output_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEvj8ZRUmRFy","executionInfo":{"status":"ok","timestamp":1668198364000,"user_tz":-60,"elapsed":100315,"user":{"displayName":"Mikołaj Zapalski","userId":"12241681530326187312"}},"outputId":"166add83-84fb-4c2d-f6f6-76ad7293c0d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["content16.jpg matejko5.jpg\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100% 528M/528M [00:02<00:00, 272MB/s]\n","-(0, 64)-\n","-(1, 128)-\n","-(2, 256)-\n","-(3, 512)-\n","Done! total time: 76.0523829460144\n","CPU times: user 563 ms, sys: 90.6 ms, total: 653 ms\n","Wall time: 1min 40s\n"]}]}]}